{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d2a5e-1fc7-460b-9082-47e5d684a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# LoadING the data\n",
    "data = pd.read_csv('HR_Analytics.csv')\n",
    "\n",
    "# Removing EmpID\n",
    "data = data.drop('EmpID', axis=1)\n",
    "\n",
    "# Converting Attrition to numeric (1 for 'Yes', 0 for 'No')\n",
    "data['Attrition'] = data['Attrition'].map({'Yes': 1, 'No': 0})\n",
    "\n",
    "# Identifying numeric and categorical columns\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "categorical_columns = data.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# One-hot encoding for converting categorical columns\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_cats = encoder.fit_transform(data[categorical_columns])\n",
    "encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Creating a new dataframe with encoded features\n",
    "encoded_df = pd.DataFrame(encoded_cats, columns=encoded_feature_names, index=data.index)\n",
    "\n",
    "# Combining numeric and encoded categorical data\n",
    "X_encoded = pd.concat([data[numeric_columns], encoded_df], axis=1)\n",
    "\n",
    "# Calculating the correlation matrix\n",
    "corr_matrix = X_encoded.corr()\n",
    "\n",
    "# Printing the full correlation matrix\n",
    "print(\"Full Correlation Matrix:\")\n",
    "print(corr_matrix)\n",
    "\n",
    "# Function to get the significant correlations\n",
    "def get_significant_correlations(corr_matrix, threshold=0.5):\n",
    "    significant_corr = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                significant_corr.append((corr_matrix.index[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "    return significant_corr\n",
    "\n",
    "# Getting and printing the significant correlations\n",
    "significant_correlations = get_significant_correlations(corr_matrix, threshold=0.5)\n",
    "print(\"\\nSignificant Correlations (|correlation| > 0.5):\")\n",
    "for var1, var2, corr in sorted(significant_correlations, key=lambda x: abs(x[2]), reverse=True):\n",
    "    print(f\"{var1} - {var2}: {corr:.2f}\")\n",
    "\n",
    "\n",
    "# Getting the correlations with Performance Rating\n",
    "perf_rating_corr = corr_matrix['PerformanceRating'].abs().sort_values(ascending=False)\n",
    "\n",
    "# Printing the top 10 correlations with Performance Rating\n",
    "print(\"Top 10 features correlated with Performance Rating:\")\n",
    "print(perf_rating_corr.head(11))  # 11 because Performance Rating's correlation with itself (1.0) will be included\n",
    "\n",
    "# Getting the correlations with Attrition\n",
    "attrition_corr = corr_matrix['Attrition'].abs().sort_values(ascending=False)\n",
    "\n",
    "# Printing the top 10 correlations with Attrition\n",
    "print(\"Top 10 features correlated with Attrition:\")\n",
    "print(attrition_corr.head(11))  # 11 because Attrition's correlation with itself (1.0) will be included\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa40cb-56c1-46f1-8de1-13b2c5edc570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predictive model\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "\n",
    "\n",
    "# loading the data\n",
    "data=pd.read_csv('HR_Analytics.csv')\n",
    "data.dtypes\n",
    "df=pd.DataFrame(data)\n",
    "\n",
    "# preprocesssing the data\n",
    "target='Attrition'\n",
    "X= df.drop(columns=[target, 'EmpID'], axis=1)\n",
    "y= df[target]\n",
    "\n",
    "\n",
    "\n",
    "# converting categorical variables with one-hot encoding\n",
    "\n",
    "encoder=OneHotEncoder(sparse_output=False)\n",
    "categorical_columns=X.select_dtypes(include=['object']).columns.tolist()\n",
    "one_hot_encoded=encoder.fit_transform(X[categorical_columns])\n",
    "one_hot_df=pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "# Concatenating the one-hot encoded dataframe with the original dataframe\n",
    "X_encoded = pd.concat([X, one_hot_df], axis=1)\n",
    "\n",
    "X_encoded = pd.concat([X, one_hot_df], axis=1)\n",
    "X_encoded = X_encoded.drop(categorical_columns, axis=1)\n",
    "\n",
    "# Combining X and y into a single dataframe\n",
    "combined_df = pd.concat([X_encoded, y], axis=1)\n",
    "\n",
    "# Drop NaN values from the combined dataframe\n",
    "combined_df = combined_df.dropna()\n",
    "\n",
    "# Splitting back into X and y\n",
    "X = combined_df.drop(columns=[target])\n",
    "y = combined_df[target]\n",
    "\n",
    "# mapping the target variable 'attrition' to binary\n",
    "y = y.map({'Yes': 1, 'No': 0})\n",
    "\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X)\n",
    "\n",
    "# Applying SMOTE to handle the data imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "the s\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "scaled_X = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Optuna objective\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.5, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 10)\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params, random_state=42)\n",
    "    \n",
    "    # Use cross-validation score\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring=make_scorer(fbeta_score, beta=2))\n",
    "    return score.mean()\n",
    "\n",
    "# Create and run Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get best parameters and train final model\n",
    "best_params = study.best_params\n",
    "best_model = xgb.XGBClassifier(**best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve and find optimal threshold\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "J = tpr - fpr\n",
    "optimal_idx = np.argmax(J)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")\n",
    "\n",
    "# Make predictions using the optimal threshold\n",
    "y_pred = (y_pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee13332-3422-49b1-b6dc-7bce1ec4b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = best_model.predict(X_train)\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy on the training and the testing sets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"The accuracy on the training set is : \", train_accuracy)\n",
    "print(\"The accuracy on the testing set is : \", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd69b7-da75-4e65-968a-6bc1c2915d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  MODEL VISUALIZATIONS\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Feature Importance Plot\n",
    "plt.figure(figsize=(20,15))\n",
    "feature_importance = best_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance in XGBoost Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Plotting the optimal threshold point \n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "plt.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', markersize=8, \n",
    "         label=f'Optimal threshold: {thresholds[optimal_idx]:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Printing the top features used to make the predictive model\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 Important Features:\")\n",
    "print(feature_importance_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
